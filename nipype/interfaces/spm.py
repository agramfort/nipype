"""
The spm module provides basic functions for interfacing with matlab and spm to 
access spm tools.

these functions include 
    
    Realign: within-modality registration

    Coregister: between modality registration
    
    Normalize: non-linear warping to standard space

    Segment: bias correction, segmentation

    Smooth: smooth with Gaussian kernel

"""
__docformat__ = 'restructuredtext'

# Standard library imports
import os
from glob import glob
from copy import deepcopy
import re

# Third-party imports
import numpy as np
from scipy.io import savemat
from scipy.signal import convolve
from scipy.special import gammaln
#from scipy.stats.distributions import gamma

# Local imports
from nipype.interfaces.base import Bunch, InterfaceResult, Interface
from nipype.utils import setattr_on_read
from nipype.externals.pynifti import load
from nipype.interfaces.matlab import fltcols, MatlabCommandLine
from nipype.utils.filemanip import (fname_presuffix, fnames_presuffix, 
                                    filename_to_list, list_to_filename)

def scans_for_fname(fname):
    """Reads a nifti file and converts it to a numpy array storing
    individual nifti volumes

    >>> scans_for_fname('mynifti.nii')
    """
    img = load(fname)
    if len(img.get_shape()) == 3:
        flist = [['%s,1'%fname]]
        return np.array([flist],dtype=object)
    else:
        n_scans = img.get_shape()[3]
        scans = []
        for sno in range(n_scans):
            scans.insert(sno,['%s,%d'% (fname, sno+1)])
        return np.array([scans],dtype=object)

def scans_for_fnames(fnames,keep4d=False):
    """Converts a list of files to a concatenated numpy array for each
    volume.

    keep4d : boolean
        keeps the entries of the numpy array as 4d files instead of
        extracting the individual volumes.
    """
    if keep4d:
        flist = [[f] for f in fnames]
        return np.array([flist],dtype=object)
    else:
        n_sess = len(fnames)
        scans = None
        for sess in range(n_sess):
            if scans is None:
                scans = scans_for_fname(fnames[sess])[0]
            else:
                scans = np.concatenate((scans,scans_for_fname(fnames[sess])[0]))
        return np.array([scans],dtype=object)

class SpmInfo(object):
    """ Return the path to the spm directory in the matlab path
    >>> print spm.spmInfo().spm_path
    """
    @setattr_on_read
    def spm_path(self):
        mlab = MatlabCommandLine()
        mlab.inputs.script_name = 'spminfo'
        mlab.inputs.script_lines = """
spm_path = spm('dir');
fprintf(1, '<PATH>%s</PATH>', spm_path);
"""
        mlab._compile_command(mfile=False)
        out = mlab.run()
        if out.runtime.returncode == 0:
            path = re.match('<PATH>(.*)</PATH>',out.runtime.stdout[out.runtime.stdout.find('<PATH>'):])
            if path is not None:
                path = path.groups()[0]
            return path
        return None

spm_info = SpmInfo()

class SpmMatlabCommandLine(MatlabCommandLine):
    """ Extends the `MatlabCommandLine` class to handle SPM specific
    formatting of matlab scripts.
    """

    mfile=True
    def gen_mfile(self, use_mfile):
        """reset the base matlab command
        """
        self.mfile = use_mfile

    def run(self):
        """Executes the SPM function using MATLAB

        Redefines the run function of the baseclass to handle
        generation of m-file job structure or a mat file job structure
        and to aggregate outputs.

        Parameters
        ----------
        mfile : boolean
            Generate an m-file that defines an SPM job. If false, a
            MAT file is created. 
        
        """
        self._compile_command()
        results = self._runner() 
        if 'MatlabScriptException' in results.runtime.stderr: 
            results.runtime.returncode = 1 
        else:
            results.outputs = self.aggregate_outputs() 
            
        return results


    def aggregate_outputs(self):
        """Collects all the outputs produced by an SPM function
        
        Virtual function that needs to be implemented by the
        subclass to collate outputs created generated by the SPM
        functionality being wrapped.
        """ 
        raise NotImplementedError
    
    def reformat_dict_for_savemat(self, contents):
        """Encloses a dict representation within hierarchical lists.

        In order to create an appropriate SPM job structure, a Python
        dict storing the job needs to be modified so that each dict
        embedded in dict needs to be enclosed as a list element.

        Examples
        --------
        >>> a = reformat_dict_for_savemat(dict(a=1,b=dict(c=2,d=3)))
        >>> print a
        [{'a': 1, 'b': [{'c': 2, 'd': 3}]}]
        
        .. notes: Need to talk to Matthew about cleaning up this code.
        """
        # Satra, I didn't change the semantics, but got rid of extraneous stuff.
        # This seems weird.  Please have a look and make sure you intend to
        # discard empty dicts. -DJC
        if type(contents) == dict:
            newdict = {}
            for key,value in contents.items():
                if type(value) == dict:
                    if value:
                        newdict[key] = self.reformat_dict_for_savemat(value)
                    # "else" - empty dicts are silently discarded here!
                else:
                    newdict[key] = value

            return [newdict]

    def generate_job(self, prefix='', contents=None):
        """ Recursive function to generate spm job specification as a string

        Parameters
        ----------
        prefix : string
            A string that needs to get  
        contents : dict
            A non-tuple Python structure containing spm job
            information gets converted to an appropriate sequence of
            matlab commands.
        """
        jobstring = ''
        if contents is None:
            return jobstring
        if type(contents) == type([]):
            for i,value in enumerate(contents):
                newprefix = "%s(%d)" % (prefix, i+1)
                jobstring += self.generate_job(newprefix, value)
            return jobstring
        if type(contents) == type({}):
            for key,value in contents.items():
                newprefix = "%s.%s" % (prefix, key)
                jobstring += self.generate_job(newprefix, value)
            return jobstring
        if type(contents) == type(np.empty(1)):
            #Assumes list of filenames embedded in a numpy array
            # does not support dicts embedded within a numpy array
            jobstring += "%s = {...\n"%(prefix)
            for item in contents[0]:
                for i,f in enumerate(item):
                    if type(f) == type(''):
                        jobstring += '\'%s\';...\n'%(f)
                    if type(f) == type(np.empty(1)):
                        jobstring += '['
                        for v in f:
                            if type(v) == type(''):
                                jobstring += '\'%s\','%v
                            else:
                                jobstring += '%s,'%str(v)
                        jobstring += '],...\n'
            jobstring += '};\n'
            return jobstring
        if type(contents) == type(''):
            jobstring += "%s = '%s';\n" % (prefix,contents)
            return jobstring
        jobstring += "%s = %s;\n" % (prefix,str(contents))
        return jobstring
    
    def make_matlab_command(self, jobtype, jobname, contents, cwd=None):
        """ generates a mfile to build job structure
        Arguments
        ---------

        cwd :
            Note that unlike calls to Popen, cwd=None will still check
            self.inputs.cwd!  Use an alternative like '.' if you need it
        """
        if cwd is None:
            cwd = self.inputs.get('cwd', '.')

        mscript = '% generated by nipype.interfaces.spm\n'
        mscript += "spm_defaults;\n\n"
        if self.mfile:
            if jobname in ['smooth','preproc','fmri_spec','fmri_est'] :
                mscript += self.generate_job('jobs{1}.%s{1}.%s(1)' % 
                                             (jobtype,jobname), contents[0])
            else:
                mscript += self.generate_job('jobs{1}.%s{1}.%s{1}' % 
                                             (jobtype,jobname), contents[0])
        else:
            jobdef = {'jobs':[{jobtype:[{jobname:self.reformat_dict_for_savemat
                                         (contents[0])}]}]}
            savemat(os.path.join(cwd,'pyjobs_%s.mat'%jobname), jobdef)
            mscript = "load pyjobs_%s;\n spm_jobman('run', jobs);\n" % jobname
        mscript += 'spm_jobman(\'run\',jobs);'
        cmdline = self.gen_matlab_command(mscript, cwd=cwd, 
                                          script_name='pyscript_%s' % jobname) 
        return cmdline, mscript


class Realign(SpmMatlabCommandLine):
    """use spm_realign for estimating within modality rigid body alignment
    
    This  routine  realigns a time-series of images acquired from the
    same  subject  using  a  least  squares approach and a 6 parameter
    (rigid  body)spatial  transformation.  The  first image  in  the
    list  specified  by  the  user  is  used as a reference  to  which
    all subsequent scans are realigned. The reference   scan   does
    not   have   to   the   the   first chronologically   and   it
    may   be   wise   to   chose   a "representative scan" in this
    role.

    The  aim is primarily to remove movement artefact in fMRI and PET
    time-series  (or  more generally longitudinal studies) . The
    headers  are modified for each of the input images, such that they
    reflect  the  relative orientations of the data. The  details  of
    the  transformation  are  displayed  in the results  window  as
    plots of translation and rotation. A set of  realignment
    parameters are saved for each session, named rp_*.txt.  After
    realignment,  the  images are resliced such that  they  match  the
    first image selected voxel-for-voxel. The  resliced  images  are
    named  the same as the originals, except that they are prefixed by
    'r'.
    
    Parameters
    ----------
    inputs : mapping
    key, value pairs that will update the Realign.inputs attributes
    see self.inputs_help() for a list of Realign.inputs attributes
    
    Attributes
    ----------
    inputs : Bunch
    a (dictionary-like) bunch of options that can be passed to 
    spm_realign via a job structure
    cmdline : string
    string used to call matlab/spm via SpmMatlabCommandLine interface
    
    

    Options
    -------

    To see optional arguments
    Realign().inputs_help()

    To see output fields
    Realign().outputs_help()

    Examples
    --------
    >>> realign = spm.Realign()
    >>> realign.inputs.infile('a.nii')
    >>> realign.run()
    """
    
    @property
    def cmd(self):
        return 'spm_realign'
        
    def inputs_help(self):
        """
        Parameters
        ----------
        
        infile: list
            list of filenames to realign
        write : bool, optional
            if True updates headers and generates
            resliced files prepended with  'r'
            if False just updates header files
            (default == True, will reslice)
        quality : float, optional
            0.1 = fastest, 1.0 = most precise
            (spm5 default = 0.9)
        fwhm : float, optional
            full width half maximum gaussian kernel 
            used to smoth images before realigning
            (spm default = 5.0)
        separation : float, optional
            separation in mm used to sample images
            (spm default = 4.0)
        register_to_mean: Bool, optional
            rtm if True uses a two pass method
            realign -> calc mean -> realign all to mean
            (spm default = False)
        weight_img : file, optional
            filename of weighting image
            if empty, no weighting 
            (spm default = None)
        wrap : list, optional
            Check if interpolation should wrap in [x,y,z]
            (spm default [0,0,0])
        interp : float, optional
            degree of b-spline used for interpolation
            (spm default = 2.0)
        write_which : list of len()==2, optional
            if write is true, 
            [inputimgs, mean]
            [2,0] reslices all images, but not mean
            [2,1] reslices all images, and mean
            [1,0] reslices imgs 2:end, but not mean
            [0,1] doesnt reslice any but generates resliced mean
        write_interp : float, optional
            degree of b-spline used for interpolation when
            writing resliced images
            (spm default = 4.0)
        write_wrap : list, optional
            Check if interpolation should wrap in [x,y,z]
            (spm default [0,0,0])
        write_mask : bool, optional
            if True, mask output image
            if False, do not mask
        flags : USE AT OWN RISK, optional
            #eg:'flags':{'eoptions':{'suboption':value}}
        """
        print self.inputs_help.__doc__

    def _populate_inputs(self):
        """ Initializes the input fields of this interface.
        """
        self.inputs = Bunch(infile=None,
                          write=True,
                          quality=None,
                          fwhm=None,
                          separation=None,
                          register_to_mean=None,
                          weight_img=None,
                          interp=None,
                          wrap=None,
                          write_which=None,
                          write_interp=None,
                          write_wrap=None,
                          write_mask=None,
                          flags=None)
        

    def get_input_info(self):
        """ Provides information about inputs as a dict
            info = [Bunch(key=string,copy=bool,ext='.nii'),...]
        """
        info = [Bunch(key='infile',copy=True)]
        return info
    
    def _parseinputs(self):
        """validate spm realign options if set to None ignore
        """
        out_inputs = []
        inputs = {}
        einputs = {'data':[],'eoptions':{},'roptions':{}}

        [inputs.update({k:v}) for k, v in self.inputs.iteritems() if v is not None ]
        for opt in inputs:
            if opt is 'flags':
                einputs.update(inputs[opt])
            if opt is 'infile':
                einputs['data'] = scans_for_fnames(filename_to_list(inputs[opt]),keep4d=True)
                continue
            if opt is 'write':
                continue
            if opt is 'quality':
                einputs['eoptions'].update({'quality': float(inputs[opt])})
                continue
            if opt is 'fwhm':
                einputs['eoptions'].update({'fwhm': float(inputs[opt])})
                continue
            if opt is 'separation':
                einputs['eoptions'].update({'sep': float(inputs[opt])})
                continue
            if opt is 'register_to_mean':
                einputs['eoptions'].update({'rtm': int(inputs[opt])})
                continue
            if opt is 'weight_img':
                einputs['eoptions'].update({'weight': inputs[opt]})
                continue
            if opt is 'interp':
                einputs['eoptions'].update({'interp': float(inputs[opt])})
                continue
            if opt is 'wrap':
                if not len(inputs[opt]) == 3:
                    raise ValueError('wrap must have 3 elements')
                einputs['eoptions'].update({'wrap': inputs[opt]})
                continue
            if opt is 'write_which':
                if not len(inputs[opt]) == 2:
                    raise ValueError('write_which must have 2 elements')
                einputs['roptions'].update({'which': inputs[opt]})
                continue
            if opt is 'write_interp':
                einputs['roptions'].update({'interp': inputs[opt]})
                continue
            if opt is 'write_wrap':
                if not len(inputs[opt]) == 3:
                    raise ValueError('write_wrap must have 3 elements')
                einputs['roptions'].update({'wrap': inputs[opt]})
                continue
            if opt is 'write_mask':
                einputs['roptions'].update({'mask': int(inputs[opt])})
                continue
                
            print 'option %s not supported'%(opt)
        return einputs

    def outputs_help(self):
        """
        Parameters
        ----------

        realigned_files :
            list of realigned files
        mean_image : 
            mean image file from the realignment process
        realignment_parameters : rp*.txt
            files containing the estimated translation and rotation
            parameters 
            """
        print self.outputs_help.__doc__

    def _compile_command(self):
        """validates spm options and generates job structure
        if mfile is True uses matlab .m file
        else generates a job structure and saves in .mat
        """
        if self.inputs.write:
            jobtype = 'estwrite'
        else:
            jobtype = 'estimate'
        self._cmdline, mscript = self.make_matlab_command('spatial', 'realign',
                                        [{'%s'%(jobtype):self._parseinputs()}])

    def aggregate_outputs(self):
        """ Initializes the output fields for this interface and then
        searches for and stores the data that go into those fields.
        """
        outputs = Bunch(realigned_files=None,
                        realignment_parameters=None,
                        mean_image=None)
        outputs.realigned_files = []
        outputs.realignment_parameters = []
        if type(self.inputs.infile) == type([]):
            filelist = self.inputs.infile
        else:
            filelist = [self.inputs.infile]
        outputs.mean_image = glob(fname_presuffix(filelist[0],prefix='mean'))[0]
        for f in filelist:
            r_file = glob(fname_presuffix(f,prefix='r',suffix='.nii',use_ext=False))
            assert len(r_file) == 1, 'No realigned file generated by SPM Realign'
            outputs.realigned_files.append(r_file[0])
            rp_file = glob(fname_presuffix(f,prefix='rp_',suffix='.txt',use_ext=False))
            assert len(rp_file) == 1, 'No realignment parameter file generated by SPM Realign'
            outputs.realignment_parameters.append(rp_file[0])
        return outputs

class Coregister(SpmMatlabCommandLine):
    """Use spm_coreg for estimating cross-modality rigid body alignment

    The  registration  method  used  here  is  based  on  work by
    Collignon  et al. The original interpolation method described in
    this  paper  has been changed in order to give a smoother cost
    function.  The images are also smoothed slightly, as is the
    histogram.  This  is  all  in  order  to  make  the cost function
    as  smooth  as possible, to give faster convergence and less
    chance of local minima. 
 
    At  the  end  of  coregistration,  the  voxel-to-voxel affine
    transformation   matrix   is   displayed,   along   with  the
    histograms  for  the images in the original orientations, and the
    final  orientations. The registered images are displayed at the
    bottom. 
 
    Registration  parameters  are  stored  in  the headers of the
    "source"  and  the  "other"  images.  These  images  are also
    resliced  to  match  the  source  image  voxel-for-voxel. The
    resliced  images  are  named the same as the originals except that
    they are prefixed by 'r'. 

    Parameters
    ----------
    inputs : mapping 
    key, value pairs that will update the Coregister.inputs attributes
    see self.inputs_help() for a list of Coregister.inputs attributes
    
    Attributes
    ----------
    inputs : Bunch
    a (dictionary-like) bunch of options that can be passed to 
    spm_coreg via a job structure
    cmdline : string
    string used to call matlab/spm via SpmMatlabSpmMatlabCommandLine interface
    
    

    Options
    -------

    To see optional arguments
    Coregister().inputs_help()

    To see output fields
    Coregister().outputs_help()

    Examples
    --------
    
    """
    
    @property
    def cmd(self):
        return 'spm_coreg'
        
    def inputs_help(self):
        """
        Parameters
        ----------
        
        target : string
            filename of nifti image to coregister to
        source : string
            filename of nifti image to coregister to the reference
        apply_to_files : list, optional
            list of filenames to apply the estimated rigid body
            transform from source to target
        write : bool, optional
            if True updates headers and generates resliced files
            prepended with  'r' if False just updates header files
            (default == True, will reslice)
        cost_function : string, optional
            maximise   or   minimise   some   objective
            function. For inter-modal    registration,    use
            Mutual   Information (mi), Normalised Mutual
            Information (nmi), or  Entropy  Correlation
            Coefficient (ecc). For within modality, you could also
            use Normalised Cross Correlation (ncc).
            (spm default = mi)
        separation : float, optional
            separation in mm used to sample images (spm default = 4.0) 
        tolerance : list of 12 floats
            The   accuracy  for  each  parameter.  Iterations stop
            when differences  between  successive  estimates are less
            than the required tolerance for each of the 12 parameters.
        fwhm : float, optional
            full width half maximum gaussian kernel used to smoth
            images before coregistering (spm default = 5.0)
        write_interp : int, optional
            degree of b-spline used for interpolation when writing
            resliced images (0 - Nearest neighbor, 1 - Trilinear, 2-7
            - degree of b-spline) (spm default = 0 - Nearest Neighbor)
        write_wrap : list, optional
            Check if interpolation should wrap in [x,y,z]
            (spm default [0,0,0])
        write_mask : bool, optional
            if True, mask output image, if False, do not mask.
            (spm default = False)
        flags : USE AT OWN RISK
        """
        print self.inputs_help.__doc__

    def _populate_inputs(self):
        """ Initializes the input fields of this interface.
        """
        self.inputs = Bunch(target=None,
                            source=None,
                            apply_to_files=None,
                            write=True,
                            cost_function=None,
                            separation=None,
                            tolerance=None,
                            fwhm=None,
                            write_interp=None,
                            write_wrap=None,
                            write_mask=None,
                            flags=None)

    def get_input_info(self):
        """ Provides information about inputs as a dict
            info = [Bunch(key=string,copy=bool,ext='.nii'),...]
        """
        info = [Bunch(key='target',copy=False),
                Bunch(key='source',copy=True),
                Bunch(key='apply_to_files',copy=True)]
        return info

    def _parseinputs(self):
        """validate spm coregister options
        if set to None ignore
        """
        out_inputs = []
        inputs = {}
        einputs = {'ref':'','source':'','other':[],'eoptions':{},'roptions':{}}

        [inputs.update({k:v}) for k, v in self.inputs.iteritems() if v is not None ]
        for opt in inputs:
            if opt is 'target':
                einputs['ref'] = list_to_filename(inputs[opt])
                continue
            if opt is 'source':
                einputs['source'] = list_to_filename(inputs[opt])
                continue
            if opt is 'apply_to_files':
                sess_scans = scans_for_fnames(filename_to_list(inputs[opt]))
                einputs['other'] = sess_scans
                continue
            if opt is 'write':
                continue
            if opt is 'cost_function':
                einputs['eoptions'].update({'cost_fun': inputs[opt]})
                continue
            if opt is 'separation':
                einputs['eoptions'].update({'sep': float(inputs[opt])})
                continue
            if opt is 'tolerance':
                einputs['eoptions'].update({'tol': inputs[opt]})
                continue
            if opt is 'fwhm':
                einputs['eoptions'].update({'fwhm': float(inputs[opt])})
                continue
            if opt is 'write_interp':
                einputs['roptions'].update({'interp': inputs[opt]})
                continue
            if opt is 'write_wrap':
                if not len(inputs[opt]) == 3:
                    raise ValueError('write_wrap must have 3 elements')
                einputs['roptions'].update({'wrap': inputs[opt]})
                continue
            if opt is 'write_mask':
                einputs['roptions'].update({'mask': int(inputs[opt])})
                continue
            if opt is 'flags':
                einputs.update(inputs[opt])
                continue
            print 'option %s not supported'%(opt)
        return einputs

    def outputs_help(self):
        """
        Parameters
        ----------

        coregistered_source :
            coregistered source file
        coregistered_files :
            coregistered files corresponding to inputs.infile
        """
        print self.outputs_help.__doc__
        
    def aggregate_outputs(self):
        outputs = Bunch(coregistered_source=None,
                        coregistered_files=None)
        c_source = glob(fname_presuffix(self.inputs.source,prefix='r',suffix='.nii',use_ext=False))
        assert len(c_source) == 1, 'No coregistered files generated by SPM Coregister'
        outputs.coregistered_source = c_source[0]
        outputs.coregistered_files = []
        if self.inputs.apply_to_files is not None:
            if type(self.inputs.apply_to_files) == type([]):
                filelist = self.inputs.apply_to_files
            else:
                filelist = [self.inputs.apply_to_files]
            for f in filelist:
                c_file = glob(fname_presuffix(f,prefix='r',suffix='.nii',use_ext=False))
                assert len(c_file) == 1, 'No coregistered file generated by SPM Coregister'
                outputs.coregistered_files.append(c_file[0])
        return outputs
        
    def _compile_command(self):
        """validates spm options and generates job structure
        if mfile is True uses matlab .m file
        else generates a job structure and saves in .mat
        """
        if self.inputs.write:
            jobtype = 'estwrite'
        else:
            jobtype = 'estimate'
        self._cmdline, mscript =self.make_matlab_command('spatial',
                                       'coreg',
                                       [{'%s'%(jobtype):self._parseinputs()}])

        
class Normalize(SpmMatlabCommandLine):
    """use spm_normalise for warping an image to a template

    Computes  the  warp  that  best  registers a source image (or
    series  of  source  images) to match a template, saving it to the
    file  imagename'_sn.mat'.  This  option  also allows the contents
    of  the imagename'_sn.mat' files to be applied to a series of
    images.
    
    Parameters
    ----------
    inputs : mapping 
    key, value pairs that will update the Normalize.inputs attributes
    see self.inputs_help() for a list of Normalize.inputs attributes
    
    Attributes
    ----------
    inputs : Bunch
    a (dictionary-like) bunch of options that can be passed to 
    spm_normalise via a job structure
    cmdline : string
    string used to call matlab/spm via SpmMatlabCommandLine interface
    
    Options
    -------

    To see optional arguments
    Normalize().inputs_help()


    Examples
    --------
    
    """
    
    @property
    def cmd(self):
        return 'spm_normalise'

    def inputs_help(self):
        """
        Parameters
        ----------
        
        template : string
            filename of nifti image to normalize to
        source : string
            filename of nifti image to normalize
        apply_to_files : list, optional
            list of filenames to apply the estimated normalization
        write : bool, optional
            if True updates headers and generates resliced files
            prepended with  'r' if False just updates header files
            (default == True, will reslice)
        source_weight : string, optional
            name of weighting image for source
        template_weight : string, optional
            name of weighting image for template
        source_image_smoothing : float, optional
        template_image_smoothing : float, optional
        affine_regularization_type : string, optional
            ICBM space template (mni), average sized template
            (size), no regularization (none)
        DCT_period_cutoff : int, optional
            Cutoff  of  DCT  bases. Only DCT bases of periods
            longer than cutoff  are  used to describe the warps.
            spm default = 25
        num_nonlinear_iterations : int, optional
            Number of iterations of nonlinear warping
            spm default = 16
        nonlinear_regularization : float, optional
            min = 0  max = 1
            spm default = 1
        write_preserve : int, optional
            Preserve  Concentrations (0): Spatially normalised images
            are not "modulated".  The  warped  images preserve the
            intensities of the original images. Preserve  Total (1):
            Spatially normalised images are "modulated" in  order
            to  preserve  the  total  amount  of signal in the
            images.   Areas   that   are   expanded  during
            warping  are correspondingly reduced in intensity.
            spm default = 0
        write_bounding_box : 6-element list, optional
        write_voxel_sizes : 3-element list, optional
        write_interp : int, optional
            degree of b-spline used for interpolation when
            writing resliced images (0 - Nearest neighbor, 1 -
            Trilinear, 2-7 - degree of b-spline)
            (spm default = 0 - Nearest Neighbor)
        write_wrap : list, optional
            Check if interpolation should wrap in [x,y,z]
            (spm default [0,0,0])
        flags : USE AT OWN RISK, optional
            #eg:'flags':{'eoptions':{'suboption':value}}
        """
        print self.inputs_help.__doc__

    def _populate_inputs(self):
        """ Initializes the input fields of this interface.
        """
        self.inputs = Bunch(template=None,
                            source=None,
                            apply_to_files=None,
                            write=True,
                            source_weight=None,
                            template_weight=None,
                            source_image_smoothing=None,
                            template_image_smoothing=None,
                            affine_regularization_type=None,
                            DCT_period_cutoff=None,
                            num_nonlinear_iterations=None,
                            nonlinear_regularization=None,
                            write_preserve=None,
                            write_bounding_box=None,
                            write_voxel_sizes=None,
                            write_interp=None,
                            write_wrap=None,
                            flags=None)

    def get_input_info(self):
        """ Provides information about inputs as a dict
            info = [Bunch(key=string,copy=bool,ext='.nii'),...]
        """
        info = [Bunch(key='source',copy=False),
                Bunch(key='apply_to_files',copy=False)]
        return info
        
    def _parseinputs(self):
        """validate spm normalize options
        if set to None ignore
        """
        out_inputs = []
        inputs = {}
        einputs = {'subj':{'resample':[]},'eoptions':{},'roptions':{}}

        [inputs.update({k:v}) for k, v in self.inputs.iteritems() if v is not None ]
        for opt in inputs:
            if opt is 'template':
                einputs['eoptions'].update({'template': scans_for_fname(list_to_filename(inputs[opt]))})
                continue
            if opt is 'source':
                einputs['subj'].update({'source': scans_for_fname(list_to_filename(inputs[opt]))})
                continue
            if opt is 'apply_to_files':
                sess_scans = scans_for_fnames(filename_to_list(inputs[opt]))
                einputs['subj']['resample'] = sess_scans
                continue
            if opt is 'write':
                if inputs[opt]:
                    if self.inputs.apply_to_files is None:
                        # SPM requires at least one file to normalize
                        # if estwrite is being used.
                        einputs['subj']['resample'] = scans_for_fname(self.inputs.source)
                continue
            if opt is 'source_weight':
                einputs['subj'].update({'wtsrc': inputs[opt]})
                continue
            if opt is 'template_weight':
                einputs['eoptions'].update({'weight': inputs[opt]})
                continue
            if opt is 'source_image_smoothing':
                einputs['eoptions'].update({'smosrc': float(inputs[opt])})
                continue
            if opt is 'template_image_smoothing':
                einputs['eoptions'].update({'smoref': float(inputs[opt])})
                continue
            if opt is 'affine_regularization_type':
                einputs['eoptions'].update({'regtype': inputs[opt]})
                continue
            if opt is 'DCT_period_cutoff':
                einputs['eoptions'].update({'cutoff': inputs[opt]})
                continue
            if opt is 'num_nonlinear_iterations':
                einputs['eoptions'].update({'nits': inputs[opt]})
                continue
            if opt is 'nonlinear_regularization':
                einputs['eoptions'].update({'reg': float(inputs[opt])})
                continue
            if opt is 'write_preserve':
                einputs['roptions'].update({'preserve': inputs[opt]})
                continue
            if opt is 'write_bounding_box':
                einputs['roptions'].update({'bb': inputs[opt]})
                continue
            if opt is 'write_voxel_sizes':
                einputs['roptions'].update({'vox': inputs[opt]})
                continue
            if opt is 'write_interp':
                einputs['roptions'].update({'interp': inputs[opt]})
                continue
            if opt is 'write_wrap':
                if not len(inputs[opt]) == 3:
                    raise ValueError('write_wrap must have 3 elements')
                einputs['roptions'].update({'wrap': inputs[opt]})
                continue
            if opt is 'flags':
                einputs.update(inputs[opt])
                continue
            print 'option %s not supported'%(opt)
        return einputs

    def _compile_command(self):
        """validates spm options and generates job structure
        if mfile is True uses matlab .m file
        else generates a job structure and saves in .mat
        """
        if self.inputs.write:
            jobtype = 'estwrite'
        else:
            jobtype = 'est'
        self._cmdline, mscript =self.make_matlab_command('spatial',
                                       'normalise',
                                       [{'%s'%(jobtype):self._parseinputs()}])

    def outputs_help(self):
        """
        Parameters
        ----------
        (all default to None)

        normalization_parameters :
            MAT file containing the normalization parameters
        normalized_source :
            normalized source file
        normalized_files :
            normalized files corresponding to inputs.apply_to_files
        """
        print self.outputs_help.__doc__
        
    def aggregate_outputs(self):
        outputs = Bunch(normalization_parameters=None,
                        normalized_source=None,
                        normalized_files=None)
        n_param = glob(fname_presuffix(self.inputs.source,suffix='_sn.mat',use_ext=False))
        assert len(n_param) == 1, 'No normalization parameter files generated by SPM Normalize'
        outputs.normalization_parameters = n_param
        n_source = glob(fname_presuffix(self.inputs.source,prefix='w',suffix='.nii',use_ext=False))
        outputs.normalized_source = n_source
        outputs.normalized_files = []
        if self.inputs.apply_to_files is not None:
            filelist = filename_to_list(self.inputs.apply_to_files)
            for f in filelist:
                n_file = glob(fname_presuffix(f,prefix='w',suffix='.nii',use_ext=False))
                assert len(n_file) == 1, 'No normalized file %s generated by SPM Normalize'%n_file
                outputs.normalized_files.append(n_file[0])
        return outputs
        
class Segment(SpmMatlabCommandLine):
    """use spm_segment to separate structural images into different
    tissue classes.

    Segment,  bias  correct  and spatially normalise - all in the same
    model.  This  function can be used for bias correcting, spatially
    normalising or segmenting your data. 

    Parameters
    ----------
    inputs : mapping 
    key, value pairs that will update the Segment.inputs attributes
    see self.inputs_help() for a list of Segment.inputs attributes
    
    Attributes
    ----------
    inputs : Bunch
    a (dictionary-like) bunch of options that can be passed to 
    spm_segment via a job structure
    cmdline : string
    string used to call matlab/spm via SpmMatlabCommandLine interface

    Options
    -------

    To see optional arguments
    Segment().inputs_help()


    Examples
    --------
    
    """
    
    @property
    def cmd(self):
        return 'spm_segment'

    def inputs_help(self):
        """
        Parameters
        ----------

        data : structural image file
            One scan per subject
        gm_output_type : 3-element list, optional
            Options to produce grey matter images: c1*.img,
            wc1*.img and mwc1*.img. None: [0,0,0], Native Space:
            [0,0,1], Unmodulated Normalised: [0,1,0], Modulated
            Normalised: [1,0,0], Native + Unmodulated Normalised:
            [0,1,1], Native + Modulated Normalised: [1,0,1],
            Native + Modulated + Unmodulated: [1,1,1], Modulated +
            Unmodulated Normalised: [1,1,0] 
        wm_output_type : 3-element list, optional
            Options to produce white matter images: c2*.img,
            wc2*.img and mwc2*.img. Same as GM options
        csf_output_type : 3-element list, optional
            Options to produce CSF images: c3*.img, wc3*.img and
            mwc3*.img. Same as GM options
        save_bias_corrected : bool, optional
            This  is  the  option  to produce a bias corrected
            version of your  image.  MR  images  are  usually
            corrupted by a smooth, spatially  varying  artifact
            that modulates the intensity of the  image  (bias).
            These  artifacts, although not usually a problem   for
            visual   inspection,   can  impede  automated
            processing  of  the images. The bias corrected version
            should have  more  uniform intensities within the
            different types of tissues.
        clean_masks : int, optional
            This  uses  a  crude  routine  for  extracting the
            brain from segmentedimages.  It  begins  by taking the
            white matter, and eroding  it  acouple  of  times to
            get rid of any odd voxels. The  algorithmcontinues  on
            to  do conditional dilations for several
            iterations,where the condition is based upon gray or
            white  matter  being  present.This  identified region
            is then used  to  clean  up  the grey and whitematter
            partitions, and has a slight influences on the CSF
            partition. Dont do cleanup (0), Light Clean (1),
            Thorough Clean (2)
        tissue_prob_maps : list of filenames, optional
            These should be maps of   grey  matter,  white  matter
            and  cerebro-spinal fluid probability.
        gaussians_per_class : 4-element list, optional
            The  number  of  Gaussians  used  to  represent the
            intensity distribution  for  each tissue class can be
            greater than one. In  other  words,  a  tissue
            probability map may be shared by several   clusters.
            The  assumption  of  a  single  Gaussian distribution
            for  each  class  does not hold for a number of
            reasons. In  particular,  a  voxel  may not be purely
            of one tissue  type,  and  instead  contain  signal
            from a number of different  tissues  (partial  volume
            effects).  Some partial volume  voxels  could fall at
            the interface between different classes,  or  they
            may fall in the middle of structures such as  the
            thalamus,  which  may  be considered as being either
            grey  or  white  matter.
        affine_regularization : string, optional
            No Affine Registration (''), ICBM space template -
            European brains (mni), ICBM space template - East
            Asian brains (eastern), Average sized template:
            (subj), No regularisation (none)
        warping_regularization : float, optional
            The   objective   function   for   registering   the
            tissue probability   maps   to   the   image  to
            process,  involves minimising  the  sum  of two
            terms. One term gives a function of  how  probable
            the  data is given the warping parameters. The  other
            is a function of how probable the parameters are, and
            provides  a  penalty for unlikely
            deformations. Smoother deformations  are  deemed  to
            be more probable. The amount of regularisation
            determines  the  tradeoff  between the terms. Pick  a
            value around one. However, if your normalised images
            appear  distorted,  then  it  may  be an idea to
            increase the amount  of  regularisation  (by  an order
            of magnitude). More regularisation   gives   smoother
            deformations,  where  the smoothness  measure  is
            determined  by the bending energy of the
            deformations.
            spm default = 1
        warp_frequency_cutoff : int, optional
            Cutoff  of  DCT  bases. Only DCT bases of periods
            longer than the  cutoff  are  used  to  describe  the
            warps.  The number actually  used  will  depend  on
            the cutoff and the field of view  of  your  image.  A
            smaller cutoff frequency will allow more  detailed
            deformations to be modelled, but unfortunately comes
            at  a  cost of greatly increasing the amount of memory
            needed, and the time taken.
        bias_regularization : float, optional
            no regularisation (0), extremely light
            regularisation (0.00001), very light regularisation
            (0.0001), light regularisation (0.001), medium
            regularisation (0.01), heavy regularisation (0.1),
            very heavy regularisation (1), extremely heavy
            regularisation (10).
        bias_fwhm : int, optional
            FWHM  of  Gaussian  smoothness  of  bias.  If  your
            intensity non-uniformity  is  very  smooth,  then
            choose a large FWHM. This  will  prevent  the
            algorithm  from trying to model out intensity
            variation due to different tissue types. 30mm to 150mm
            cutoff: (30-150 in steps of 10), No correction (inf).
        sampling_distance : float, optional
            The   approximate   distance   between  sampled
            points  when estimating  the  model
            parameters. Smaller values use more of the data, but
            the procedure is slower.
        mask_image : filename, optional
            The  segmentation  can be masked by an image that
            conforms to the  same space as the images to be
            segmented. If an image is selected,  then  it  must
            match the image(s) voxel-for voxel, and  have the same
            voxel-to-world mapping. Regions containing a  value
            of  zero  in  this  image  do  not  contribute when
            estimating the various parameters. 
        flags : USE AT OWN RISK, optional
            #eg:'flags':{'eoptions':{'suboption':value}}
        """
        print self.inputs_help.__doc__

    def _populate_inputs(self):
        """ Initializes the input fields of this interface.
        """
        self.inputs = Bunch(data=None,
                            gm_output_type=None,
                            wm_output_type=None,
                            csf_output_type=None,
                            save_bias_corrected=None,
                            clean_masks=None,
                            tissue_prob_maps=None,
                            gaussians_per_class=None,
                            affine_regularization=None,
                            warping_regularization=None,
                            warp_frequency_cutoff=None,
                            bias_regularization=None,
                            bias_fwhm=None,
                            sampling_distance=None,
                            mask_image=None,
                            flags=None)

    def get_input_info(self):
        """ Provides information about inputs as a dict
            info = [Bunch(key=string,copy=bool,ext='.nii'),...]
        """
        info = [Bunch(key='data',copy=False)]
        return info
    
    def _parseinputs(self):
        """validate spm segment options
        if set to None ignore
        """
        out_inputs = []
        inputs = {}
        einputs = {'data':[],'output':{},'opts':{}}

        [inputs.update({k:v}) for k, v in self.inputs.iteritems() if v is not None ]
        for opt in inputs:
            if opt is 'data':
                if type(inputs[opt]) == type([]):
                    sess_scans = scans_for_fnames(inputs[opt])
                else:
                    sess_scans = scans_for_fname(inputs[opt])
                einputs['data'] = sess_scans
                continue
            if opt is 'gm_output_type':
                einputs['output']['GM'] = inputs[opt]
                continue
            if opt is 'wm_output_type':
                einputs['output']['WM'] = inputs[opt]
                continue
            if opt is 'csf_output_type':
                einputs['output']['CSF'] = inputs[opt]
                continue
            if opt is 'save_bias_corrected':
                einputs['output']['biascor'] = int(inputs[opt])
                continue
            if opt is 'clean_masks':
                einputs['output']['cleanup'] = inputs[opt]
                continue
            if opt is 'tissue_prob_maps':
                if type(inputs[opt]) == type([]):
                    sess_scans = scans_for_fnames(inputs[opt])
                else:
                    sess_scans = scans_for_fname(inputs[opt])
                einputs['opts']['tpm'] = sess_scans
                continue
            if opt is 'gaussians_per_class':
                einputs['opts']['ngaus'] = inputs[opt]
                continue
            if opt is 'affine_regularization':
                einputs['opts']['regtype'] = inputs[opt]
                continue
            if opt is 'warping_regularization':
                einputs['opts']['warpreg'] = inputs[opt]
                continue
            if opt is 'warp_frequency_cutoff':
                einputs['opts']['warpco'] = inputs[opt]
                continue
                continue
            if opt is 'bias_regularization':
                einputs['opts']['biasreg'] = inputs[opt]
                continue
            if opt is 'bias_fwhm':
                einputs['opts']['biasfwhm'] = inputs[opt]
                continue
            if opt is 'sampling_distance':
                einputs['opts']['samp'] = inputs[opt]
                continue
            if opt is 'mask_image':
                einputs['opts']['msk'] = scans_for_fname(inputs[opt])
                continue
            if opt is 'flags':
                einputs.update(inputs[opt])
                continue
            print 'option %s not supported'%(opt)
        return einputs

    def _compile_command(self):
        """validates spm options and generates job structure
        if mfile is True uses matlab .m file
        else generates a job structure and saves in .mat
        """
        self._cmdline, mscript =self.make_matlab_command('spatial',
                                                       'preproc',
                                                       [self._parseinputs()])

    def outputs_help(self):
        """
        Parameters
        ----------
        (all default to None)

        native_class_images :
            native space images for each of the three tissue types
        normalized_class_images :
            normalized class images for each of the three tissue 
            types
        modulated_class_images :
            modulated, normalized class images for each of the three tissue
            types
        modulated_input_images :
            modulated version of input image
        transformation_mat :
            Transformation file for normalizing image
        inverse_transformation_mat :
            Transformation file for inverse normalizing an image
        """
        print self.outputs_help.__doc__
        
    def aggregate_outputs(self):
        outputs = Bunch(native_class_images=None,
                        normalized_class_images=None,
                        modulated_class_images=None,
                        modulated_input_image=None,
                        transformation_mat=None,
                        inverse_transformation_mat=None)
        f = self.inputs.data
        m_file = glob(fname_presuffix(f,prefix='m',suffix='.nii',use_ext=False))
        outputs.modulated_input_image = m_file
        c_files = glob(fname_presuffix(f,prefix='c*',suffix='.nii',use_ext=False))
        outputs.native_class_images = c_files
        wc_files = glob(fname_presuffix(f,prefix='wc*',suffix='.nii',use_ext=False))
        outputs.normalized_class_images = wc_files
        mwc_files = glob(fname_presuffix(f,prefix='mwc*',suffix='.nii',use_ext=False))
        outputs.modulated_class_images = mwc_files
        t_mat = glob(fname_presuffix(f,suffix='_seg_sn.mat',use_ext=False))
        outputs.transformation_mat = t_mat
        invt_mat = glob(fname_presuffix(f,suffix='_seg_inv_sn.mat',use_ext=False))
        outputs.inverse_transformation_mat = invt_mat
        return outputs

class Smooth(SpmMatlabCommandLine):
    """use spm_smooth for 3D Gaussian smoothing of image volumes.

    This  is  for  smoothing (or convolving) image volumes with a
    Gaussian  kernel  of  a  specified  width.  It  is  used as a
    preprocessing  step  to  suppress  noise  and  effects due to
    residual  differences  in functional and gyral anatomy during
    inter-subject averaging.

    Parameters
    ----------
    inputs : mapping 
    key, value pairs that will update the Smooth.inputs attributes
    see self.inputs_help() for a list of Smooth.inputs attributes
    
    Attributes
    ----------
    inputs : Bunch
    a (dictionary-like) bunch of options that can be passed to 
    spm_smooth via a job structure
    cmdline : string
    string used to call matlab/spm via SpmMatlabCommandLine interface

    Options
    -------

    To see optional arguments
    Smooth().inputs_help()


    Examples
    --------
    
    """
    
    @property
    def cmd(self):
        return 'spm_smooth'

    def inputs_help(self):
        """
        Parameters
        ----------
        
        infile : list
            list of filenames to apply smoothing
        fwhm : 3-list, optional
            list of fwhm for each dimension
        data_type : int, optional
            Data-type  of output images. SAME (0) indicates the
            same datatype as the original images.
            spm default = 0
        flags : USE AT OWN RISK, optional
            #eg:'flags':{'eoptions':{'suboption':value}}
        """
        print self.inputs_help.__doc__

    def _populate_inputs(self):
        """ Initializes the input fields of this interface.
        """
        self.inputs = Bunch(infile=None,
                            fwhm=None,
                            data_type=None,
                            flags=None)

    def get_input_info(self):
        """ Provides information about inputs as a dict
            info = [Bunch(key=string,copy=bool,ext='.nii'),...]
        """
        info = [Bunch(key='infile',copy=False)]
        return info
        
    def _parseinputs(self):
        """validate spm smooth options
        if set to None ignore
        """
        out_inputs = []
        inputs = {}
        einputs = {'data':[],'fwhm':[],'dtype':0}

        [inputs.update({k:v}) for k, v in self.inputs.iteritems() if v is not None ]
        for opt in inputs:
            if opt is 'infile':
                sess_scans = scans_for_fnames(filename_to_list(inputs[opt]))
                einputs['data'] = sess_scans
                continue
            if opt is 'fwhm':
                einputs['fwhm'] = inputs[opt]
                continue
            if opt is 'data_type':
                einputs['dtype'] = inputs[opt]
                continue
            if opt is 'flags':
                einputs.update(inputs[opt])
                continue
            print 'option %s not supported'%(opt)
        return einputs

    def _compile_command(self):
        """validates spm options and generates job structure
        if mfile is True uses matlab .m file
        else generates a job structure and saves in .mat
        """
        self._cmdline, mscript =self.make_matlab_command('spatial',
                                                       'smooth',
                                                       [self._parseinputs()])

    def outputs_help(self):
        """
        Parameters
        ----------
        (all default to None)
        
        smoothed_files :
            smooth files corresponding to inputs.infile
        """
        print self.outputs_help.__doc__
        
    def aggregate_outputs(self):
        outputs = Bunch(smoothed_files=None)
        outputs.smoothed_files = []
        if type(self.inputs.infile) == type([]):
            filelist = self.inputs.infile
        else:
            filelist = [self.inputs.infile]
        for f in filelist:
            s_file = glob(fname_presuffix(f,prefix='s',suffix='.nii',use_ext=False))
            assert len(s_file) == 1, 'No smoothed file generated by SPM Smooth'
            outputs.smoothed_files.append(s_file[0])
        return outputs

class Level1Design(SpmMatlabCommandLine):
    """Generate an SPM design matrix

    Statistical  analysis  of  fMRI  data  uses a mass-univariate
    approach  based on General Linear Models (GLMs). It comprises
    the  following  steps  (1)  specification  of  the GLM design
    matrix,  fMRI  data files and filtering (2) estimation of GLM
    paramaters  using  classical  or  Bayesian approaches and (3)
    interrogation  of  results  using contrast vectors to produce
    Statistical  Parametric  Maps (SPMs) or Posterior Probability
    Maps (PPMs).

    The  design  matrix  defines  the experimental design and the
    nature  of  hypothesis  testing to be implemented. The design
    matrix  has  one  row  for  each scan and one column for each
    effect  or  explanatory  variable. (eg. regressor or stimulus
    function).  You  can  build  design  matrices  with separable
    session-specific  partitions.  Each partition may be the same
    (in  which  case  it is only necessary to specify it once) or
    different.

    Responses  can  be  either  event- or epoch related, the only
    distinction  is  the  duration  of  the  underlying  input or
    stimulus  function.  Mathematically  they are both modeled by
    convolving  a  series  of delta (stick) or box functions (u),
    indicating  the  onset  of  an  event  or epoch with a set of
    basis  functions. These basis functions model the hemodynamic
    convolution,  applied  by  the  brain,  to  the  inputs. This
    convolution  can  be first-order or a generalized convolution
    modeled   to  second  order  (if  you  specify  the  Volterra
    option).  The  same  inputs are used by the Hemodynamic model
    or   Dynamic   Causal  Models  which  model  the  convolution
    explicitly in terms of hidden state variables.
 
    Basis  functions  can  be used to plot estimated responses to
    single  events  once  the  parameters  (i.e.  basis  function
    coefficients)  have  been  estimated. The importance of basis
    functions  is that they provide a graceful transition between
    simple  fixed  response  models (like the box-car) and finite
    impulse  response  (FIR)  models,  where  there  is one basis
    function  for  each  scan  following an event or epoch onset.
    The  nice  thing  about  basis  functions,  compared  to  FIR
    models,  is that data sampling and stimulus presentation does
    not  have  to  be synchronized thereby allowing a uniform and
    unbiased sampling of peri-stimulus time.
    
    Event-related  designs  may  be  stochastic or deterministic.
    Stochastic  designs  involve  one  of a number of trial-types
    occurring   with   a   specified  probability  at  successive
    intervals   in   time.   These  probabilities  can  be  fixed
    (stationary   designs)   or   time-dependent   (modulated  or
    non-stationary  designs).  The  most efficient designs obtain
    when  the  probabilities  of  every  trial  type are equal. A
    critical  issue  in  stochastic designs is whether to include
    null  events If you wish to estimate the evoked response to a
    specific  event  type  (as opposed to differential responses)
    then  a  null  event  must  be  included  (even  if it is not
    modeled explicitly).
 
    In  SPM,  analysis  of  data from multiple subjects typically
    proceeds  in  two  stages  using  models at two 'levels'. The
    'first  level'  models are used to implement a within-subject
    analysis.  Typically there will be as many first level models
    as  there  are subjects. Analysis proceeds as described using
    the   'Specify  first  level'  and  'Estimate'  options.  The
    results  of  these  analyses  can  then be presented as 'case
    studies'.  More often, however, one wishes to make inferences
    about  the  population  from  which  the subjects were drawn.
    This  is an example of a 'Random-Effects (RFX) analysis' (or,
    more   properly,  a  mixed-effects  analysis).  In  SPM,  RFX
    analysis   is   implemented   using  the  'summary-statistic'
    approach  where contrast images from each subject are used as
    summary   measures  of  subject  responses.  These  are  then
    entered as data into a 'second level' model.
    
    Parameters
    ----------
    inputs : mapping 
    key, value pairs that will update the Level1Design.inputs attributes
    see self.inputs_help() for a list of Level1Design.inputs attributes
    
    Attributes
    ----------
    inputs : Bunch
    a (dictionary-like) bunch of options that can be passed to 
    spm_smooth via a job structure
    cmdline : string
    string used to call matlab/spm via SpmMatlabCommandLine interface
    
    

    Options
    -------

    To see optional arguments
    Level1Design().inputs_help()


    Examples
    --------
    
    """
    
    @property
    def cmd(self):
        return 'spm_fmri_design'

    def inputs_help(self):
        """
        Parameters
        ----------
        
        spmmat_dir : string
            directory in which to store the SPM.mat file
        timing_units : string
            units for specification of onsets or blocks
            (scans or secs) 
        interscan_interval : float (in secs)
            Interscan  interval,  TR, (specified in seconds). This
            is the time  between  acquiring  a  plane of one
            volume and the same plane  in  the  next  volume.  It
            is  assumed to be constant throughout.

        microtime_resolution : float (in secs)
            The  microtime resolution, t, is the number of
            time-bins per scan used when building regressors.
            spm default = 16
        microtime_onset : float (in secs)
            The  microtime  onset, is the first time-bin at which
            the regressors  are  resampled to coincide with data
            acquisition. If  onset  =  1  then the regressors will be
            appropriate for the first   slice.   If   you  want
            to  temporally  realign  the regressors  so  that they
            match responses in the middle slice then  make  onset  =
            t/2  (assuming  there  is a negligible gap between
            volume acquisitions).
            Do not change the default settings for the above two
            parameters unless you have a long TR.
        session_info : list of dicts
            Stores session specific information

            Session parameters

            nscan : int
                Number of scans in a session
            scans : list of filenames
                A single 4D nifti file or a list of 3D nifti files
            hpf : float
                High pass filter cutoff
                SPM default = 128 secs
            condition_info : mat filename or list of dicts 
                Stores condition specific information

                MAT file contents

                If  you  have multiple conditions then entering the details a
                condition  at  a time is very inefficient. This option can be
                used  to  load  all  the  required information in one go. You
                will  first  need  to  create  a  *.mat  file  containing the
                relevant information.
         
                This  *.mat file must include the following cell arrays (each
                1  x  n) :  names,  onsets and durations. eg. names=cell(1,5),
                onsets=cell(1,5),          durations=cell(1,5),          then
                names{2}='SSent-DSpeak',     onsets{2}=[3    5    19    222],
                durations{2}=[0  0  0 0], contain the required details of the
                second  condition. These cell arrays may be made available by
                your  stimulus  delivery  program,  eg.  COGENT. The duration
                vectors  can  contain  a  single  entry  if the durations are
                identical for all events.
                                                                                                         
                Time  and  Parametric  effects can also be included. For time
                modulation  include  a  cell  array  (1  x n) called tmod. It
                should  have  a  have  a  single  number in each cell. Unused
                cells  may  contain  either  a 0 or be left empty. The number
                specifies  the  order  of  time  modulation  from 0 = No Time
                Modulation  to  6  = 6th Order Time Modulation. eg. tmod{3} =
                1, modulates the 3rd condition by a linear time effect.
                                                                                                         
                For  parametric  modulation  include a structure array, which
                is  up  to 1 x n in size, called pmod. n must be less than or
                equal  to  the  number of cells in the names/onsets/durations
                cell  arrays.  The structure array pmod must have the fields:
                name,  param and poly. Each of these fields is in turn a cell
                array  to  allow  the  inclusion  of  one  or more parametric
                effects  per  column  of the design. The field name must be a
                cell  array  containing  strings.  The  field param is a cell
                array  containing  a  vector  of  parameters.  Remember  each
                parameter  must  be  the  same  length  as  its corresponding
                onsets   vector.   The  field  poly  is  a  cell  array  (for
                consistency)  with  each  cell  containing  a  single  number
                specifying the order of the polynomial expansion from 1 to 6.
                                                                                                            
                Note  that each condition is assigned its corresponding entry
                in  the  structure  array  (condition 1 parametric modulators
                are  in  pmod(1),  condition  2  parametric modulators are in
                pmod(2),   etc.   Within   a  condition  multiple  parametric
                modulators  are  accessed via each fields cell arrays. So for
                condition  1,  parametric  modulator  1  would  be defined in
                pmod(1).name{1},  pmod(1).param{1},  and  pmod(1).poly{1}.  A
                second  parametric modulator for condition 1 would be defined
                as  pmod(1).name{2}, pmod(1).param{2} and pmod(1).poly{2}. If
                there  was  also a parametric modulator for condition 2, then
                remember  the  first  modulator for that condition is in cell
                array     1:     pmod(2).name{1},    pmod(2).param{1},    and
                pmod(2).poly{1}.   If   some,  but  not  all  conditions  are
                parametrically  modulated,  then the non-modulated indices in
                the  pmod  structure  can  be  left  blank.  For  example, if
                conditions  1  and  3 but not condition 2 are modulated, then
                specify  pmod(1)  and pmod(3). Similarly, if conditions 1 and
                2  are  modulated  but  there are 3 conditions overall, it is
                only necessary for pmod to be a 1 x 2 structure array.
                                                                                                            
                EXAMPLE:
                Make an empty pmod structure: 
                pmod = struct('name',{''},'param',{},'poly',{});
                Specify one parametric regressor for the first condition: 
                  pmod(1).name{1}  = 'regressor1';
                  pmod(1).param{1} = [1 2 4 5 6];
                  pmod(1).poly{1}  = 1;
                Specify 2 parametric regressors for the second condition: 
                  pmod(2).name{1}  = 'regressor2-1';
                  pmod(2).param{1} = [1 3 5 7]; 
                  pmod(2).poly{1}  = 1;
                  pmod(2).name{2}  = 'regressor2-2';
                  pmod(2).param{2} = [2 4 6 8 10];
                  pmod(2).poly{2}  = 1;
                                                                                                            
                The   parametric   modulator  should  be  mean  corrected  if
                appropriate.  Unused structure entries should have all fields
                left empty.

                Condition parameters

                name : string
                    Name of condition
                onset : list of numbers
                    Onset times of each event/block
                duration: float or list
                    Specify   the   event   durations.  Epoch  and
                    event-related responses  are  modeled  in exactly
                    the  same  way  but  by specifying their
                    different  durations. Events are specified with  a
                    duration  of 0. If you enter a single number for
                    the durations  it will be assumed that all trials
                    conform to this duration.  If you have multiple
                    different durations, then the number must match
                    the number of onset times.
                tmod : int
                    This  option  allows  for  the
                    characterisation of linear or nonlinear  time 
                    effects.  For  example, 1st order modulation would
                    model  the  stick functions and a linear change of
                    the stick  function  heights  over  time. Higher
                    order modulation will   introduce  further
                    columns  that  contain  the  stick functions
                    scaled by time squared, time cubed etc.
                pmod : list of dicts
                    Model  interractions  with  user  specified
                    parameters. This allows  nonlinear  effects
                    relating to some other measure to be modelled in
                    the design matrix.

                    Parametric modulator parameters

                    name : string
                        Name of the parametric modulator
                    param : list
                        Numerical values of the parameter. One per
                        each event of the condition.
                    poly : int
                        For  example,  1st  order  modulation
                        would  model the stick functions  and  a
                        linear change of the stick function heights
                        over   different   values  of  the  parameter.
                        Higher  order modulation  will  introduce
                        further columns that contain the stick
                        functions scaled by parameter squared, cubed
                        etc.

            regressor_info : mat/txt filename or list of dicts 
                Stores regressor specific information

                MAT/TXT file contents

                You  will  first  need  to  create  a *.mat file
                containing a matrix  R  or  a  *.txt  file
                containing the regressors. Each column  of  R
                will  contain  a different regressor. When SPM
                creates  the  design  matrix the regressors will
                be named R1, R2, R3, ..etc.
 
                Regressor parameters

                name : string
                    Name of regressor
                val : list 
                    List of values for the regressor

        factor_info : list of dicts
            Stores factor specific information

            Factor parameters

            name : string
                Name of factor (use condition name)
            levels: int
                Number of levels for the factor

        bases : dict {'name':{'basesparam1':val,...}}
            name : string
                Name of basis function (hrf, fourier, fourier_han,
                gamma, fir)
                
                hrf :
                    derivs : 2-element list
                        Model  HRF  Derivatives. The canonical HRF combined with time
                        and  dispersion derivatives comprise an 'informed' basis set,
                        as  the  shape  of  the  canonical  response  conforms to the
                        hemodynamic   response   that   is   commonly  observed.  The
                        incorporation  of  the derivate terms allow for variations in
                        subject-to-subject  and  voxel-to-voxel  responses.  The time
                        derivative  allows the peak response to vary by plus or minus
                        a  second  and  the dispersion derivative allows the width of
                        the  response  to  vary.  The  informed basis set requires an
                        SPM{F}  for  inference.  T-contrasts  over just the canonical
                        are  perfectly  valid  but  assume constant delay/dispersion.
                        The  informed  basis  set  compares  favourably  with eg. FIR
                        bases on many data sets. No derivatives: [0,0],
                        Time derivatives : [1,0], Time and Dispersion
                        derivatives: [1,1]
                fourier, fourier_han, gamma, fir:
                    length : int
                        Post-stimulus window length (in seconds)
                    order : int
                        Number of basis functions
        volterra_expansion_order : int
            Generalized convolution of inputs (U) with basis set
            (bf). Do not model interactions (1) or model
            interactions (2)
            SPM default = 1
        global_intensity_normalization : string
            Global intensity normalization (scaling or none)
            SPM default  = none
        mask_image : filename
            Specify  an  image  for  explicitly  masking  the
            analysis. 
        model_serial_correlations : string
            Serial  correlations  in  fMRI  time  series  due  to
            aliased biorhythms  and unmodelled neuronal activity
            can be accounted for  using  an  autoregressive  AR(1)
            model during Classical (ReML) parameter estimation.
            AR(1) or none
            SPM default = AR(1) 
        flags : USE AT OWN RISK
               #eg:'flags':{'eoptions':{'suboption':value}}
        """
        print self.inputs_help.__doc__

    def _populate_inputs(self):
        """ Initializes the input fields of this interface.
        """
        self.inputs = Bunch(spmmat_dir=None,
                            timing_units=None,
                            interscan_interval=None,
                            microtime_resolution=None,
                            microtime_onset=None,
                            session_info=None,
                            factor_info=None,
                            bases=None,
                            volterra_expansion_order=None,
                            global_intensity_normalization=None,
                            mask_image=None,
                            model_serial_correlations=None,
                            flags=None)
        
    def _parseinputs(self):
        """validate spm normalize options
        if set to None ignore
        """
        out_inputs = []
        inputs = {}
        einputs = {'dir':'','timing':{},'sess':[],'fact':{},'bases':{},
                   'volt':{},'global':{},'mask':{}}

        [inputs.update({k:v}) for k, v in self.inputs.iteritems() if v is not None ]
        for opt in inputs:
            if opt is 'spmmat_dir':
                einputs['dir'] = np.array([[[str(inputs[opt])]]],dtype=object)
                continue
            if opt is 'timing_units':
                einputs['timing'].update(units=inputs[opt])
                continue
            if opt is 'interscan_interval':
                einputs['timing'].update(RT=inputs[opt])
                continue
            if opt is 'microtime_resolution':
                einputs['timing'].update(fmri_t=inputs[opt])
                continue
            if opt is 'microtime_onset':
                einputs['timing'].update(fmri_t0=inputs[opt])
                continue
            if opt is 'session_info':
                einputs['sess'] = inputs[opt]
                continue
            if opt is 'factor_info':
                einputs['fact'] = inputs[opt]
                continue
            if opt is 'bases':
                einputs['bases'] = inputs[opt]
                continue
            if opt is 'volterra_expansion_order':
                einputs['volt'] = inputs[opt]
                continue
            if opt is 'global_intensity_normalization':
                einputs['global'] = inputs[opt]
                continue
            if opt is 'mask_image':
                einputs['mask'] = inputs[opt]
                continue
            if opt is 'model_serial_correlations':
                einputs['cvi'] = inputs[opt]
                continue
            if opt is 'flags':
                einputs.update(inputs[opt])
                continue
            print 'option %s not supported'%(opt)
            if einputs['dir'] == '':
                einputs['dir'] = np.array([[[str(self.inputs.get('cwd','.'))]]],dtype=object)
        return einputs

    def _compile_command(self):
        """validates spm options and generates job structure
        if mfile is True uses matlab .m file
        else generates a job structure and saves in .mat
        """
        self._cmdline, mscript =self.make_matlab_command('stats',
                                                       'fmri_spec',
                                                       [self._parseinputs()])

    def outputs_help(self):
        """
            Parameters
            ----------
            (all default to None)

            spm_mat_file:
                SPM mat file
        """
        print self.outputs_help.__doc__
        
    def aggregate_outputs(self):
        outputs = Bunch(spm_mat_file=None)
        spm = glob(os.path.join(self.inputs.get('cwd','.'),'SPM.mat'))
        assert len(spm) == 1, 'No spm mat files generated by SPM Estimate'
        outputs.spm_mat_file = spm[0]
        return outputs
    
class EstimateModel(SpmMatlabCommandLine):
    """use spm_spm to estimate the parameters of a model

    Model  parameters  can  be  estimated using classical (ReML -
    Restricted  Maximum Likelihood) or Bayesian algorithms. After
    parameter  estimation,  the  RESULTS  button  can  be used to
    specify  contrasts  that  will produce Statistical Parametric
    Maps  (SPMs)  or Posterior Probability Maps (PPMs) and tables
    of statistics.

    Parameters
    ----------
    
    inputs : mapping 
    key, value pairs that will update the EstimateModel.inputs attributes
    see self.inputs_help() for a list of EstimateModel.inputs attributes
    
    Attributes
    ----------
    
    inputs : Bunch
    a (dictionary-like) bunch of options that can be passed to 
    spm_spm via a job structure
    cmdline : string
    string used to call matlab/spm via SpmMatlabCommandLine interface

    Options
    -------

    To see optional arguments
    EstimateModel().inputs_help()


    Examples
    --------
    
    """
    
    @property
    def cmd(self):
        return 'spm_spm'

    def inputs_help(self):
        """
            Parameters
            ----------
            
            spm_design_file : filename
                Filename containing absolute path to SPM.mat
            estimation_method: dict
                There  are  three  possible  estimation  procedures  for fMRI
                models  (1)  classical  (ReML)  estimation of first or second
                level  models,  (2) Bayesian estimation of first level models
                and  (3)  Bayesian  estimation of second level models. Option
                (2)  uses  a  Variational Bayes (VB) algorithm that is new to
                SPM5.  Option  (3)  uses  the  Empirical Bayes algorithm with
                global shrinkage priors that was also in SPM2.

                {'Classical' : 1}
                    Model  parameters  are  estimated  using  Restricted  Maximum
                    Likelihood   (ReML).   This  assumes  the  error  correlation
                    structure  is the same at each voxel.
                {'Bayesian2' : 1}
                    Bayesian  estimation  of  2nd  level models. This option uses
                    the  Empirical  Bayes  algorithm with global shrinkage priors
                    that  was  previously  implemented in SPM2. Use of the global
                    shrinkage  prior  embodies  a  prior  belief that, on average
                    over  all  voxels,  there is no net experimental effect. Some
                    voxels  will  respond  negatively  and some positively with a
                    variability  determined  by  the  prior precision. This prior
                    precision  can  be  estimated  from  the data using Empirical
                    Bayes.
                {'Bayesian' : dict}
                    Model  parameters are estimated using Variational Bayes (VB).
                    This  allows  you  to  specify  spatial priors for regression
                    coefficients  and  regularised  voxel-wise  AR(P)  models for
                    fMRI   noise   processes.  The  algorithm  does  not  require
                    functional  images  to be spatially smoothed. Estimation will
                    take  about  5 times longer than with the classical approach.
                    This is why VB is not the default estimation
                    option.
                    
                    USE IF YOU KNOW HOW TO SPECIFY PARAMETERS
                    
            flags : USE AT OWN RISK
                #eg:'flags':{'eoptions':{'suboption':value}}
        """
        print self.inputs_help.__doc__

    def _populate_inputs(self):
        """ Initializes the input fields of this interface.
        """
        self.inputs = Bunch(spm_design_file=None,
                            estimation_method=None,
                            flags=None)
        
    def get_input_info(self):
        """ Provides information about inputs as a dict
            info = [Bunch(key=string,copy=bool,ext='.nii'),...]
        """
        info = [Bunch(key='spm_design_file',copy=True)]
        return info
    
    def _parseinputs(self):
        """validate spm normalize options
        if set to None ignore
        """
        out_inputs = []
        inputs = {}
        einputs = {'spmmat':'','method':{}}

        [inputs.update({k:v}) for k, v in self.inputs.iteritems() if v is not None ]
        for opt in inputs:
            if opt is 'spm_design_file':
                einputs['spmmat'] = np.array([[[str(inputs[opt])]]],dtype=object)
                continue
            if opt is 'estimation_method':
                einputs['method'].update(inputs[opt])
                continue
            if opt is 'flags':
                einputs.update(inputs[opt])
                continue
            print 'option %s not supported'%(opt)
        return einputs

    def _compile_command(self):
        """validates spm options and generates job structure
        if mfile is True uses matlab .m file
        else generates a job structure and saves in .mat
        """
        self._cmdline, mscript = self.make_matlab_command('stats',
                                                       'fmri_est',
                                                       [self._parseinputs()])
    
    def outputs_help(self):
        """
            Parameters
            ----------
            (all default to None)

            mask_image:
                binary brain mask within which estimation was
                performed
            beta_images:
                Parameter estimates for each column of the design matrix
            residual_image:
                Mean-squared image of the residuals from each time point
            RPVimage:
                ??? XXX ???
            spm_mat_file:
                Updated SPM mat file
        """
        print self.outputs_help.__doc__
        
    def aggregate_outputs(self):
        pth, fname = os.path.split(self.inputs.spm_design_file)
        outputs = Bunch(mask_image=None,
                        beta_images=None,
                        residual_image=None,
                        RPVimage=None,
                        spm_mat_file=None)
        mask = glob(os.path.join(pth,'mask.*'))
        assert len(mask) == 2, 'No mask image file generated by SPM Estimate'
        outputs.mask_image = mask
        betas = glob(os.path.join(pth,'beta*.*'))
        assert len(betas) >= 2, 'No beta image files generated by SPM Estimate'
        outputs.beta_images = betas
        resms = glob(os.path.join(pth,'ResMS.*'))
        assert len(resms) == 2, 'No residual image files generated by SPM Estimate'
        outputs.residual_image = resms
        rpv = glob(os.path.join(pth,'RPV.*'))
        assert len(rpv) == 2, 'No residual image files generated by SPM Estimate'
        outputs.rpv_image = rpv
        spm = glob(os.path.join(pth,'SPM.mat'))
        assert len(spm) == 1, 'No spm mat files generated by SPM Estimate'
        outputs.spm_mat_file = spm[0]
        return outputs

class SpecifyModel(Interface):
    """Makes a model specification SPM specific

    Parameters
    ----------
    
    inputs : mapping 
    key, value pairs that will update the SpecifyModel.inputs attributes
    see self.inputs_help() for a list of SpecifyModel.inputs attributes
    
    Attributes
    
    inputs : Bunch
    a (dictionary-like) bunch of options that can be passed to 
    spm_spm via a job structure
    cmdline : string
    string used to call matlab/spm via SpmMatlabCommandLine interface

    Options

    To see optional arguments
    SpecifyModel().inputs_help()


    Examples
    --------
    
    """
    
    def __init__(self, *args, **inputs):
        self._populate_inputs()
        self.inputs.update(**inputs)

    def inputs_help(self):
        """
            Parameters
            ----------

            subject_id : string or int
                Subject identifier used as a parameter to the
                subject_info_func.
            subject_info_func : function
                Returns subject specific condition information. If all
                subjects had the same stimulus presentation schedule,
                then this function can return the same structure
                independent of the subject. This function must retun a
                list of dicts with the list length equal to the number of
                sessions. The dicts should contain the following
                information.

                conditions : list of names
                onsets : lists of onsets corresponding to each
                    condition
                durations : lists of durations corresponding to each
                    condition. Should be left to a single 0 if all
                    events are being modeled as impulses.
                amplitudes : lists of amplitudes for each event. This
                    is ignored by SPM
                tmod : lists of conditions that should be temporally
                   modulated. Should default to None if not being used.
                pmod : list of dicts corresponding to conditions
                    name : name of parametric modulator
                    param : values of the modulator
                    poly : degree of modulation
                regressors : list of dicts or matfile
                    names : list of names corresponding to each
                       column. Should be None if automatically
                       assigned.
                    values : lists of values for each regressor
                    matfile : MAT-file containing names and a matrix
                        called R
            realignment_parameters : list of files
                Realignment parameters returned by some motion
                correction algorithm. Assumes that each file is a text
                file containing a row of translation and rotation
                parameters.
            outlier_files : list of files
                A list of files containing outliers that should be
                tossed. One file per session.
            functional_runs : list of files
                List of data files for model. One file per session
            input_units : string
                Units of event onsets and durations (secs or scans) as
                returned by the subject_info_func
            output_units : string
                Units of event onsets and durations (secs or scans) as
                sent to SPM design
            polynomial_order : int
                Number of polynomial functions used to model high pass
                filter. 
            concatenate_runs : boolean
                Allows concatenating all runs to look like a single
                expermental session.
            time_repetition : float
                Time between the start of one volume to the start of
                the next image volume. If a clustered acquisition is
                used, then this should be the time between the start
                of acquisition of one cluster to the start of
                acquisition of the next cluster.

            Sparse and clustered-sparse specific options
            
            time_acquisition : float
                Time in seconds to acquire a single image volume
            volumes_in_cluster : int
                If number of volumes in a cluster is greater than one,
                then a sparse-clustered acquisition is being assumed.
            model_hrf : boolean
                Whether to model hrf for sparse clustered analysis
            stimuli_as_impulses : boolean
                Whether to treat each stimulus to be impulse like. If
                not, the stimuli are convolved with their respective
                durations.
            scan_onset : float
                Start of scanning relative to onset of run in
                secs. default = 0 
        """
        print self.inputs_help.__doc__

    def _populate_inputs(self):
        """ Initializes the input fields of this interface.
        """
        self.inputs = Bunch(subject_id=None,
                            subject_info_func=None,
                            realignment_parameters=None,
                            outlier_files=None,
                            functional_runs=None,
                            input_units=None,
                            output_units=None,
                            concatenate_runs=None,
                            time_repetition=None,
                            time_acquisition=None,
                            volumes_in_cluster=None,
                            model_hrf=None,
                            stimuli_as_impulses=True,
                            scan_onset=0.)
        
    def outputs_help(self):
        """
            Parameters
            ----------
            (all default to None)

            session_info:
                Python dict storing session info for input to
                spm.Level1Design.inputs.session_info 
        """
        print self.outputs_help.__doc__

    def _scaletimings(self,timelist,input_units=None,output_units=None):
        if input_units is None:
            input_units = self.inputs.input_units
        if output_units is None:
            output_units = self.inputs.output_units
        if input_units==output_units:
            self._scalefactor = 1.
        if (input_units == 'scans') and (output_units == 'secs'):
            if self.inputs.volumes_in_cluster > 1:
                raise NotImplementedError("cannot scale timings if times are scans and acquisition is clustered")
            else:
                self._scalefactor = self.inputs.time_repetition
        if (input_units == 'secs') and (output_units == 'scans'):
            self._scalefactor = 1./self.inputs.time_repetition

        if self._scalefactor > 1:
            timelist = [self._scalefactor*t for t in timelist]
        else:
            timelist = [round(self._scalefactor*t) for t in timelist]
            
        return timelist
    
    def _gcd(self,a,b):
        """Returns the greates common divisor of two integers

        uses Euclid's algorithm
        """
        while b > 0: a,b = b, a%b
        return a

    def _spm_hrf(self,RT,P=[],fMRI_T=16):
        """ python implementation of spm_hrf
        see spm_hrf for implementation details

        % RT   - scan repeat time
        % p    - parameters of the response function (two gamma
        % functions)
        % defaults  (seconds)
        %	p(0) - delay of response (relative to onset)	   6
        %	p(1) - delay of undershoot (relative to onset)    16
        %	p(2) - dispersion of response			   1
        %	p(3) - dispersion of undershoot			   1
        %	p(4) - ratio of response to undershoot		   6
        %	p(5) - onset (seconds)				   0
        %	p(6) - length of kernel (seconds)		  32
        %
        % hrf  - hemodynamic response function
        % p    - parameters of the response function

        >>> _spm_hrf(2)
        array([  0.00000000e+00,   8.65660810e-02,   3.74888236e-01,
         3.84923382e-01,   2.16117316e-01,   7.68695653e-02,
         1.62017720e-03,  -3.06078117e-02,  -3.73060781e-02,
        -3.08373716e-02,  -2.05161334e-02,  -1.16441637e-02,
        -5.82063147e-03,  -2.61854250e-03,  -1.07732374e-03,
        -4.10443522e-04,  -1.46257507e-04])
        """
        p     = np.array([6,16,1,1,6,0,32],dtype=float)
        if len(P)>0:
            p[0:len(P)] = P

        _spm_Gpdf = lambda x,h,l: np.exp(h*np.log(l)+(h-1)*np.log(x)-(l*x)-gammaln(h))
        # modelled hemodynamic response function - {mixture of Gammas}
        dt    = RT/float(fMRI_T)
        u     = np.arange(0,int(p[6]/dt+1)) - p[5]/dt
        # the following code using scipy.stats.distributions.gamma
        # doesn't return the same result as the spm_Gpdf function
        # hrf   = gamma.pdf(u,p[0]/p[2],scale=dt/p[2]) - gamma.pdf(u,p[1]/p[3],scale=dt/p[3])/p[4]
        hrf   = _spm_Gpdf(u,p[0]/p[2],dt/p[2]) - _spm_Gpdf(u,p[1]/p[3],dt/p[3])/p[4]
        idx   = np.arange(0,int((p[6]/RT)+1))*fMRI_T
        hrf   = hrf[idx]
        hrf   = hrf/np.sum(hrf)
        return hrf
        
    def _gen_regress(self,i_onsets,i_durations,nscans,bplot=False):
        """Generates a regressor for a sparse/clustered-sparse acquisition

           see Ghosh et al. (2009) OHBM 2009
        """
        if bplot:
            import matplotlib.pyplot as plt
        TR = np.round(self.inputs.time_repetition*1000)  # in ms
        if self.inputs.time_acquisition is None:
            TA = TR # in ms
        else:
            TA = np.round(self.inputs.time_acquisition*1000) # in ms
        nvol = self.inputs.volumes_in_cluster
        SCANONSET = np.round(self.inputs.scan_onset*1000)
        total_time = TR*(nscans-nvol)/nvol + TA*nvol + SCANONSET
        SILENCE = TR-TA*nvol
        dt = TA/10.;
        durations  = np.round(np.array(i_durations)*1000)
        if len(durations) == 1:
            durations = durations*np.ones((len(i_onsets)))
        onsets = np.round(np.array(i_onsets)*1000)
        dttemp = self._gcd(TA,self._gcd(SILENCE,TR))
        if dt < dttemp:
            if dttemp % dt != 0:
                dt = self._gcd(dttemp,dt)
        if dt < 1:
            raise Exception("Time multiple less than 1 ms")
        print "Setting dt = %d ms\n" % dt
        npts = int(total_time/dt)
        times = np.arange(0,total_time,dt)*1e-3
        timeline = np.zeros((npts))
        timeline2 = np.zeros((npts))
        hrf = self._spm_hrf(dt*1e-3)
        for i,t in enumerate(onsets):
            idx = int(t/dt)
            timeline2[idx] = 1
            if bplot:
                plt.subplot(4,1,1)
                plt.plot(times,timeline2)
            if not self.inputs.stimuli_as_impulses:
                if durations[i] == 0:
                    durations[i] = TA*nvol
                stimdur = np.ones((int(durations[i]/dt)))
                timeline2 = convolve(timeline2,stimdur)[0:len(timeline2)]
            timeline += timeline2
            timeline2[:] = 0
        if bplot:
            plt.subplot(4,1,2)
            plt.plot(times,timeline)
        if self.inputs.model_hrf:
            timeline = convolve(timeline,hrf)[0:len(timeline)]
        if bplot:
            plt.subplot(4,1,3)
            plt.plot(times,timeline)
        # sample timeline
        timeline2 = np.zeros((npts))
        reg = []
        for i,trial in enumerate(np.arange(nscans)/nvol):
            scanstart = int((SCANONSET + trial*TR + (i%nvol)*TA)/dt)
            #print total_time/dt, SCANONSET, TR, TA, scanstart, trial, i%2, int(TA/dt)
            scanidx = scanstart+np.arange(int(TA/dt))
            timeline2[scanidx] = np.max(timeline)
            reg.insert(i,np.mean(timeline[scanidx]))
        if bplot:
            plt.subplot(4,1,3)
            plt.plot(times,timeline2)
            plt.subplot(4,1,4)
            plt.bar(np.arange(len(reg)),reg,width=0.5)
        return reg

    def _cond_to_regress(self,info,nscans):
        reg = []
        regnames = info.conditions
        for i,c in enumerate(info.conditions):
            reg.insert(i,self._gen_regress(self._scaletimings(info.onsets[i],output_units='secs'),
                                           self._scaletimings(info.durations[i],output_units='secs'),
                                           nscans))
            # need to deal with temporal and parametric modulators
        # for sparse-clustered acquisitions enter T1-effect regressors
        nvol = self.inputs.volumes_in_cluster
        if nvol > 1:
            for i in range(nvol-1):
                treg = np.zeros((nscans/nvol,nvol))
                treg[:,i] = 1
                reg.insert(len(reg),treg.ravel().tolist())
        return reg,regnames
    
    def _generate_clustered_design(self,infolist):
        """
        """
        infoout = deepcopy(infolist)
        for i,info in enumerate(infolist):
            infoout[i].conditions = None
            infoout[i].onsets = None
            infoout[i].durations = None
            if info.conditions is not None:
                img = load(self.inputs.functional_runs[i])
                nscans = img.get_shape()[3]
                reg,regnames = self._cond_to_regress(info,nscans)
                if infoout[i].regressors is None:
                    infoout[i].regressors = []
                    infoout[i].regressor_names = []
                else:
                    if infoout[i].regressor_names is None:
                        infoout[i].regressor_names = ['R%d'%j for j in range(len(infoout[i].regressors))] 
                for j,r in enumerate(reg):
                    regidx = len(infoout[i].regressors)
                    infoout[i].regressor_names.insert(regidx,regnames[j])
                    infoout[i].regressors.insert(regidx,r)
        return infoout
    
    def _generate_standard_design(self,infolist,
                                  functional_runs=None,
                                  realignment_parameters=None,
                                  outliers=None):
        """ Generates a standard design matrix paradigm
        """
        sessinfo = []
        for i,info in enumerate(infolist):
            sessinfo.insert(i,dict(cond=[]))
            if info.conditions is not None:
                for cid,cond in enumerate(info.conditions):
                    sessinfo[i]['cond'].insert(cid,dict())
                    sessinfo[i]['cond'][cid]['name']  = info.conditions[cid]
                    sessinfo[i]['cond'][cid]['onset'] = self._scaletimings(info.onsets[cid])
                    sessinfo[i]['cond'][cid]['duration'] = self._scaletimings(info.durations[cid])
                    if info.tmod is not None:
                        sessinfo[i]['cond'][cid]['tmod'] = info.tmod[cid]
                    if (info.pmod is not None) and info.pmod.has_key(cid+1):
                        sessinfo[i]['cond'][cid]['pmod'] = []
                        for j in range(len(info.pmod[cid+1].name)):
                            sessinfo[i]['cond'][cid]['pmod'].insert(j,dict())
                        for key,data in info.pmod[cid+1].iteritems():
                            for k,val in enumerate(data):
                                sessinfo[i]['cond'][cid]['pmod'][k][key] = val
            sessinfo[i]['regress']= []
            if info.regressors is not None:
                for j,r in enumerate(info.regressors):
                    sessinfo[i]['regress'].insert(j,dict(name='',val=[]))
                    if info.regressor_names is not None:
                        sessinfo[i]['regress'][j]['name'] = info.regressor_names[j]
                    else:
                        sessinfo[i]['regress'][j]['name'] = 'UR%d'%(j+1)
                    sessinfo[i]['regress'][j]['val'] = info.regressors[j]
            if functional_runs is not None:
                sessinfo[i]['scans'] = scans_for_fnames(filename_to_list(functional_runs[i]),keep4d=False)
            else:
                raise Exception("No functional data information provided for model")
        if realignment_parameters is not None:
            for i,rp in enumerate(realignment_parameters):
                mc = realignment_parameters[i]
                for col in range(mc.shape[1]):
                    colidx = len(sessinfo[i]['regress'])
                    sessinfo[i]['regress'].insert(colidx,dict(name='',val=[]))
                    sessinfo[i]['regress'][colidx]['name'] = 'Realign%d'%(col+1)
                    sessinfo[i]['regress'][colidx]['val']  = mc[:,col].tolist()
        if outliers is not None:
            for i,out in enumerate(outliers):
                numscans = sessinfo[i]['scans'][0].shape[0] 
                for j,scanno in enumerate(out):
                    if True:
                        colidx = len(sessinfo[i]['regress'])
                        sessinfo[i]['regress'].insert(colidx,dict(name='',val=[]))
                        sessinfo[i]['regress'][colidx]['name'] = 'Outlier%d'%(j+1)
                        sessinfo[i]['regress'][colidx]['val']  = np.zeros((1,numscans))[0].tolist()
                        sessinfo[i]['regress'][colidx]['val'][int(scanno)] = 1
                    else:
                        cid = len(sessinfo[i]['cond'])
                        sessinfo[i]['cond'].insert(cid,dict())
                        sessinfo[i]['cond'][cid]['name'] = "O%d"%(j+1)
                        sessinfo[i]['cond'][cid]['onset'] = self._scaletimings([scanno])
                        sessinfo[i]['cond'][cid]['duration'] = [0]
        return sessinfo
    
    def _concatenate_info(self,infolist):
        nscans = []
        for i,f in enumerate(self.inputs.functional_runs):
            img = load(f)
            nscans.insert(i,img.get_shape()[3])
            # now combine all fields into 1
            # names,onsets,durations,amplitudes,pmod,tmod,regressor_names,regressors
        infoout = infolist[0]
        for i,info in enumerate(infolist[1:]):
                #info.[conditions,tmod] remain the same
            if info.onsets is not None:
                for j,val in enumerate(info.onsets):
                    if self.inputs.input_units == 'secs':
                        infoout.onsets[j].extend((np.array(info.onsets[j])+
                                                  self.inputs.time_repetition*sum(nscans[0:(i+1)])).tolist())
                    else:
                        infoout.onsets[j].extend((np.array(info.onsets[j])+sum(nscans[0:(i+1)])).tolist())
                for j,val in enumerate(info.durations):
                    if len(val) > 1:
                        infoout.durations[j].extend(info.durations[j])
                if info.pmod is not None:
                    for key,data in info.pmod.items():
                        for j,v in enumerate(data.param):
                            infoout.pmod[key].param[j].extend(v)
            if info.regressors is not None:
                #assumes same ordering of regressors across different
                #runs and the same names for the regressors
                for j,v in enumerate(info.regressors):
                    infoout.regressors[j].extend(info.regressors[j])
            #insert session regressors
            if True:
                if infoout.regressors is None:
                    infoout.regressors = []
                onelist = np.zeros((1,sum(nscans)))
                onelist[0,sum(nscans[0:(i)]):sum(nscans[0:(i+1)])] = 1
                infoout.regressors.insert(len(infoout.regressors),onelist.tolist()[0])
            else:
                infoout.conditions.append('Session%d'%i)
                infoout.onsets.insert(len(infoout.onsets),[sum(nscans[0:i])])
                infoout.durations.insert(len(infoout.durations),[nscans[i]])
                if infoout.tmod is not None:
                    infoout.tmod.insert(len(infoout.tmod),0)
            # insert outliers as new regressors
        return [infoout],nscans
    
    def _generate_design(self):
        infolist = self.inputs.subject_info_func(self.inputs.subject_id)
        if self.inputs.concatenate_runs:
            infolist,nscans = self._concatenate_info(infolist)
            functional_runs = [self.inputs.functional_runs]
        else:
            functional_runs = self.inputs.functional_runs
        realignment_parameters = []
        if self.inputs.realignment_parameters is not None:
            rpfiles = filename_to_list(self.inputs.realignment_parameters)
            realignment_parameters.insert(0,np.loadtxt(rpfiles[0]))
            for rpf in rpfiles[1:]:
                mc = np.loadtxt(rpf)
                if self.inputs.concatenate_runs:
                    realignment_parameters[0] = np.concatenate((realignment_parameters[0],mc))
                else:
                    realignment_parameters.insert(len(realignment_parameters),mc)
        outliers = []
        if self.inputs.outlier_files is not None:
            outfiles = filename_to_list(self.inputs.outlier_files)
            outliers.insert(0,np.loadtxt(outfiles[0]))
            for i,rpf in enumerate(outfiles[1:]):
                out = np.loadtxt(rpf)
                if self.inputs.concatenate_runs:
                    if len(out)>0:
                        outliers[0] = np.concatenate((outliers[0],
                                                      (np.array(out)+sum(nscans[0:(i+1)])).tolist()))
                else:
                    outliers.insert(len(outliers),out)
        if self.inputs.volumes_in_cluster is not None:
            infolist = self._generate_clustered_design(infolist)
        sessinfo = self._generate_standard_design(infolist,
                                                  functional_runs=functional_runs,
                                                  realignment_parameters=realignment_parameters,
                                                  outliers=outliers)
        return sessinfo
    
    def aggregate_outputs(self):
        outputs = Bunch(session_info=self._generate_design())
        return outputs

    def run(self):
        """
        """
        runtime = Bunch(returncode=0,
                        messages=None,
                        errmessages=None)
        outputs=self.aggregate_outputs()
        return InterfaceResult(deepcopy(self), runtime, outputs=outputs)
    

class EstimateContrast(SpmMatlabCommandLine):
    """use spm_contrasts to estimate contrasts of interest

    Parameters
    ----------
    
    inputs : mapping 
    key, value pairs that will update the EstimateContrast.inputs attributes
    see self.inputs_help() for a list of EstimateContrast.inputs attributes
    
    Attributes
    ----------
    
    inputs : Bunch
    a (dictionary-like) bunch of options that can be passed to 
    spm_spm via a job structure
    cmdline : string
    string used to call matlab/spm via SpmMatlabCommandLine interface

    Options
    -------

    To see optional arguments
    EstimateContrast().inputs_help()


    Examples
    --------
    
    """
    
    @property
    def cmd(self):
        return 'spm_contrast'

    def inputs_help(self):
        """
            Parameters
            ----------
            
            spm_mat_file : filename
                Filename containing absolute path to SPM.mat
            contrasts : list of dicts
                List of contrasts with each dict containing: 'name',
                'stat', [condition list], [weight list], [session
                list]. if session list is None, all sessions are
                used. 
            beta_images:
                Parameter estimates for each column of the design matrix
            residual_image:
                Mean-squared image of the residuals from each time point
            RPVimage:
                ??? XXX ???
            ignore_derivs : boolean
                Whether to ignore derivatives from contrast
                estimation. default : True
            flags : USE AT OWN RISK
                #eg:'flags':{'eoptions':{'suboption':value}}
        """
        print self.inputs_help.__doc__

    def _populate_inputs(self):
        """ Initializes the input fields of this interface.
        """
        self.inputs = Bunch(spm_mat_file=None,
                            contrasts=None,
                            beta_images=None,
                            residual_image=None,
                            RPVimage=None,
                            ignore_derivs=True,
                            flags=None)
        
    def get_input_info(self):
        """ Provides information about inputs as a dict
            info = [Bunch(key=string,copy=bool,ext='.nii'),...]
        """
        info = [Bunch(key='spm_mat_file',copy=True),
                Bunch(key='beta_images',copy=False),
                Bunch(key='residual_image',copy=False),
                Bunch(key='RPVimage',copy=False),
                ]
        return info
    
    def _parseinputs(self):
        """validate spm normalize options
        if set to None ignore
        """
        out_inputs = []
        inputs = {}
        einputs = {'spmmat':'','method':{}}

        [inputs.update({k:v}) for k, v in self.inputs.iteritems() if v is not None ]
        for opt in inputs:
            if opt is 'spm_mat_file':
                einputs['spmmat'] = np.array([[[str(inputs[opt])]]],dtype=object)
                continue
            if opt is 'contrasts':
                continue
            if opt is 'beta_images':
                continue
            if opt is 'residual_image':
                continue
            if opt is 'RPVimage':
                continue
            if opt is 'flags':
                continue
            if opt is 'ignore_derivs':
                continue
            print 'option %s not supported'%(opt)
        return einputs

    def _compile_command(self):
        """validates spm options and generates job structure
        if mfile is True uses matlab .m file
        else generates a job structure and saves in .mat
        """
        contrasts = []
        for i,cont in enumerate(self.inputs.contrasts):
            contrasts.insert(i,Bunch(name=cont[0],
                                     stat=cont[1],
                                     conditions=cont[2],
                                     weights=cont[3]))

        script  = "% generated by nipype.interfaces.spm\n"
        script += "spm_defaults;\n"
        script += "jobs{1}.stats{1}.con.spmmat  = {'%s'};\n" % self.inputs.spm_mat_file
        script += "load(jobs{1}.stats{1}.con.spmmat{:});\n"
        script += "SPM.swd = '%s';\n" % self.inputs.get('cwd','.')
        script += "save(jobs{1}.stats{1}.con.spmmat{:},'SPM');\n"
        script += "names = SPM.xX.name;\n"
        if self.inputs.ignore_derivs:
            script += "pat = 'Sn\([0-9*]\) (.*)\*bf\(1\)|Sn\([0-9*]\) .*\*bf\([2-9]\)|Sn\([0-9*]\) (.*)';\n"
        else:
            script += "pat = 'Sn\([0-9*]\) (.*)\*bf\([0-9]\)|Sn\([0-9*]\) (.*)';\n"
        script += "t = regexp(names,pat,'tokens');\n"
        script += "for i0=1:numel(t),condnames{i0}='';if ~isempty(t{i0}{1}),condnames{i0} = t{i0}{1}{1};end;end;\n"
        # BUILD CONTRAST SESSION STRUCTURE
        for i,contrast in enumerate(contrasts):
            if contrast.stat == 'T':
                script += "consess{%d}.tcon.name   = '%s';\n" % (i+1,contrast.name)
                script += "consess{%d}.tcon.convec = zeros(1,numel(names));\n" % (i+1)
                for c0,cond in enumerate(contrast.conditions):
                    script += "idx = strmatch('%s',condnames,'exact');\n" % (cond)
                    script += "consess{%d}.tcon.convec(idx) = %f;\n" % (i+1,contrast.weights[c0])
            elif contrast.stat == 'F':
                script += "consess{%d}.fcon.name   =  '%s';\n" % (i+1,contrast[0])
                for cl0,fcont in enumerate(contrast.conditions):
                    script += "consess{%d}.fcon.convec{%d} = zeros(1,numel(names));\n" % (i+1,cl0+1)
                    for c0,cond in enumerate(fcont):
                        script += "idx = strmatch('%s',condnames,'exact');\n" % (cond)
                        script += "consess{%d}.fcon.convec{%d}(idx) = %f;\n" % (i+1,cl0+1,contrast.weights[cl0][c0])
            else:
                raise Exception("Contrast Estimate: Unknown stat %s"%contrast.stat)
        script += "jobs{1}.stats{1}.con.consess = consess;\n"
        script += "spm_jobman('run',jobs);"
        self._cmdline = self.gen_matlab_command(script,
                                                cwd=self.inputs.get('cwd','.'),
                                                script_name='pyscript_contrastestimate') 
    
    def outputs_help(self):
        """
            Parameters
            ----------
            (all default to None)

            con_images:
                contrast images from a t-contrast
                binary brain mask within which estimation was
                performed
            beta_images:
                Parameter estimates for each column of the design matrix
            residual_image:
                Mean-squared image of the residuals from each time point
            RPVimage:
                ??? XXX ???
            spm_mat_file:
                Updated SPM mat file
        """
        print self.outputs_help.__doc__
        
    def aggregate_outputs(self):
        pth, fname = os.path.split(self.inputs.spm_mat_file)
        outputs = Bunch(con_images=None,
                        spmT_images=None,
                        ess_images=None,
                        spmF_images=None)
                                          
        con = glob(os.path.join(pth,'con*.*'))
        if len(con)>0:
            outputs.con_images = sorted(con)
        spmt = glob(os.path.join(pth,'spmT*.*'))
        if len(spmt)>0:
            outputs.spmT_images = sorted(spmt)
        ess = glob(os.path.join(pth,'ess*.*'))
        if len(ess)>0:
            outputs.ess_images = sorted(ess)
        spmf = glob(os.path.join(pth,'spmF*.*'))
        if len(spmf)>0:
            outputs.spmF_images = sorted(spmf)
        return outputs
